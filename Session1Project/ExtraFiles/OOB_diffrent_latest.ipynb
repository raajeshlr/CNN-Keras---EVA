{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OOB_diffrent_latest",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sw4nNhM8mN2z",
        "outputId": "6f2d1878-9424-4d45-e693-e4d9f4a12d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "#tf.enable_eager_execution()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VeLRe4cBmbu_",
        "colab": {}
      },
      "source": [
        "\n",
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9Men1oDmesN",
        "colab": {}
      },
      "source": [
        "train_features = train_features.astype('float32')/255\n",
        "test_features = test_features.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhAmhrsOLQRq",
        "colab_type": "code",
        "outputId": "95df6271-ee39-4024-ce7f-33b55efa67e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_features.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcQrJ2EnKUvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_crop(input_image,padding_pixels=4,random_crop_size=(32,32)):\n",
        "    assert input_image.shape[2]==3\n",
        "\n",
        "    #pad for 4 pixels\n",
        "    img = cv2.copyMakeBorder(input_image,padding_pixels,padding_pixels,padding_pixels,padding_pixels,cv2.BORDER_REPLICATE)\n",
        "    height , width =img.shape[0],img.shape[1]\n",
        "    dy,dx = random_crop_size\n",
        "    x = np.random.randint(0,width - dx + 1)\n",
        "    y = np.random.randint(0,height - dy + 1)\n",
        "    return img[y:(y+dy),x:(x+dx),:]\n",
        "\n",
        "#lambda_feat = lambda t : random_crop(t)\n",
        "tr_seq = list(range(len(train_features)))\n",
        "#te_seq = list(range(len(test_features)))\n",
        "train_func = lambda i:random_crop(train_features[i])\n",
        "train_features = list(map(train_func,tr_seq))\n",
        "\n",
        "train_features = np.asarray(train_features)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q5EigjQNmhWU",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.3, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        if pixel_level:\n",
        "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "        else:\n",
        "            c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf7MTd6B-rYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_mean = np.mean(train_features,axis=(0,1,2))\n",
        "x_train_std = np.std(train_features,axis=(0,1,2))\n",
        "x_test_mean = np.mean(test_features,axis=(0,1,2))\n",
        "x_test_std = np.std(test_features,axis=(0,1,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3w6Bris4vod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = (train_features-x_train_mean)/x_train_std\n",
        "test_features = (test_features-x_test_mean)/x_test_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2tOuprdCpqSB",
        "colab": {}
      },
      "source": [
        "datagen_aug = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=0.5,rotation_range=45,preprocessing_function=get_random_eraser())\n",
        "# datagen_aug.mean = np.array(x_train_mean, dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "# datagen_aug.std = np.array(x_train_std, dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen_aug.fit(train_features)\n",
        "train_generator = datagen_aug.flow(train_features,train_labels,batch_size=256)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AMi1CeDsqUyv",
        "colab": {}
      },
      "source": [
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "test_datagen.fit(test_features)\n",
        "test_generator = test_datagen.flow(test_features,test_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZVue0KrdH2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# aug_img=np.zeros((256,32,32,3))\n",
        "# train_gen = train_generator.next()\n",
        "# aug_img[:256]= train_gen[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B_Gdr7lP-rq",
        "colab_type": "code",
        "outputId": "fef88f9d-faab-400f-eb07-a30abcfa3479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "import h5py\n",
        "f = h5py.File('aug_img.hdf5', 'w')\n",
        "d = f.create_dataset('dataset', (1200000,32,32,3),chunks=(256,32,32,3))\n",
        "l = f.create_dataset('labels',(1200000,10),chunks=(256,10))\n",
        "\n",
        "\n",
        "batch_size=256\n",
        "EPOCHS =24\n",
        "aug_lbl=[]\n",
        "tmp=0\n",
        "for i in range(EPOCHS):\n",
        "  print(\"tmp \", tmp)\n",
        "  for j in range(len(train_generator)):\n",
        "    initial = j*batch_size+tmp\n",
        "    train_gen = train_generator.next()\n",
        "    if(j == 195 ):\n",
        "      final += 80\n",
        "      d[initial:final]= train_gen[0]\n",
        "      l[initial:final]= train_gen[1]\n",
        "      continue \n",
        "      \n",
        "    \n",
        "    final = initial + 256\n",
        "    d[initial:final]= train_gen[0]\n",
        "    l[initial:final]= train_gen[1]\n",
        "  \n",
        "  tmp = final\n",
        "\n",
        "# f.close()\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9e4eaac33bee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aug_img.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1200000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to truncate a file which is already open)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Za7Xax0zyZc2"
      },
      "source": [
        "#Resnet Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xp2uGvCQcTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time, math\n",
        "def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "#initializer = tf.keras.initializers.glorot_normal(seed=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B41YNxNtCUwa",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import regularizers\n",
        "def ResNetBlock(input_layer, channels,stride=1):\n",
        "  \n",
        "  bn_1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(input_layer)\n",
        "  activation_layer_b1 = tf.keras.layers.Activation('relu')(bn_1)\n",
        "  if(stride==2):\n",
        "    block_layer_1 = tf.keras.layers.Conv2D(channels, (3,3) ,padding='same',kernel_initializer=init_pytorch,use_bias=False,\n",
        "                                           kernel_regularizer=regularizers.l2(0.005))(activation_layer_b1)\n",
        "    block_layer_1= tf.keras.layers.MaxPooling2D()(block_layer_1)\n",
        "  else:\n",
        "    block_layer_1 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',kernel_initializer=init_pytorch,use_bias=False,\n",
        "                                           kernel_regularizer=regularizers.l2(0.005))(activation_layer_b1)\n",
        "  \n",
        "#   bn_2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(block_layer_1)\n",
        "#   activation_layer_b1 = tf.keras.layers.Activation('relu')(bn_1)\n",
        "  #one_layer = tf.keras.layers.Conv2D(channels-(channels//2), (1, 1), padding='same')(block_layer_1)\n",
        "  #drp_r = tf.keras.layers.SpatialDropout2D(0.05)(block_layer_1)\n",
        "  \n",
        "  bn_2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(block_layer_1)\n",
        "  activation_layer_b2 = tf.keras.layers.Activation('relu')(bn_2) \n",
        "  block_layer_2 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',kernel_initializer=init_pytorch,use_bias=False,kernel_regularizer=regularizers.l2(0.005))(activation_layer_b2)\n",
        "   \n",
        "  \n",
        "  return block_layer_2\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7zl45v4UBTff",
        "outputId": "1d21d989-3245-4555-e777-09dad0439cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# from tf.keras.layers import Input, add, GlobalAveragePooling2D, Dense\n",
        "#from tf.keras.models import Model\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "\n",
        "x1 = tf.keras.layers.Conv2D(32, (3, 3),padding='same',kernel_initializer=init_pytorch,use_bias=False,kernel_regularizer=regularizers.l2(0.01))(inputs)   #32x32 \n",
        "activation_x1 = tf.keras.layers.Activation('relu')(x1)\n",
        "bn1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x1)\n",
        "\n",
        "# x2 = tf.keras.layers.Conv2D(64, (3, 3),padding='same',kernel_initializer=initializer)(bn1)   #32x32 \n",
        "# activation_x2 = tf.keras.layers.Activation('relu')(x2)\n",
        "# bn2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x2)\n",
        "\n",
        "# x3 = tf.keras.layers.Conv2D(64, (3, 3),padding='same')(activation_x2)   #32x32 \n",
        "# activation_x3 = tf.keras.layers.Activation('relu')(x3)\n",
        "# bn3 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(activation_x3)\n",
        "\n",
        "# mx_p= tf.keras.layers.MaxPooling2D()(bn1)\n",
        "\n",
        "\n",
        "\n",
        "##block 1\n",
        "\n",
        "blk1 = ResNetBlock(bn1,32,stride=2)  ##32x32\n",
        "mxp_1 = tf.keras.layers.MaxPooling2D()(bn1)\n",
        "#one_blk_1 = tf.keras.layers.Conv2D(32, (1, 1), padding='same',strides=2)(bn1)\n",
        "z1 = tf.keras.layers.add([blk1,mxp_1])\n",
        "\n",
        "blk1_c = ResNetBlock(z1,32)\n",
        "z1_c = tf.keras.layers.add([blk1_c,z1])\n",
        "\n",
        "drp_1 = tf.keras.layers.SpatialDropout2D(0.05)(z1_c)\n",
        "\n",
        "\n",
        "##block 2\n",
        "\n",
        "blk2 = ResNetBlock(drp_1,64)\n",
        "one_blk = tf.keras.layers.Conv2D(64, (1, 1), padding='same')(drp_1)\n",
        "z2 = tf.keras.layers.add([blk2,one_blk])\n",
        "\n",
        "blk2_c = ResNetBlock(z2,64)\n",
        "z2_c = tf.keras.layers.add([blk2_c,z2])\n",
        "\n",
        "\n",
        "drp_2 = tf.keras.layers.SpatialDropout2D(0.05)(z2_c)\n",
        "\n",
        "##block3\n",
        "\n",
        "blk3 = ResNetBlock(drp_2,128,stride=2)\n",
        "\n",
        "one_blk_1 = tf.keras.layers.Conv2D(128,(1, 1), padding='same')(drp_2)\n",
        "mxp_2 = tf.keras.layers.MaxPooling2D()(one_blk_1)\n",
        "\n",
        "\n",
        "z3 = tf.keras.layers.add([blk3,mxp_2])\n",
        "\n",
        "blk3_c = ResNetBlock(z3,128)\n",
        "z3_c = tf.keras.layers.add([blk3_c,z3])\n",
        "\n",
        "\n",
        "drp_3 = tf.keras.layers.Dropout(0.05)(z3_c)\n",
        "#block4\n",
        "\n",
        "blk4 = ResNetBlock(drp_3,256,stride=2)\n",
        "\n",
        "one_blk_2 = tf.keras.layers.Conv2D(256, (1, 1), padding='same')(drp_3)\n",
        "mxp_3 = tf.keras.layers.MaxPooling2D()(one_blk_2)\n",
        "\n",
        "z4 = tf.keras.layers.add([blk4,mxp_3])\n",
        "\n",
        "blk4_c = ResNetBlock(z4,256)\n",
        "z4_c = tf.keras.layers.add([blk4_c,z4])\n",
        "# drp_4 = tf.keras.layers.Dropout(0.05)(z4_c)\n",
        "\n",
        "# #block5\n",
        "# blk5 = ResNetBlock(drp_4,128,stride=2)\n",
        "# one_blk_3 = tf.keras.layers.Conv2D(128, (1, 1), padding='same',strides=2)(drp_4)\n",
        "# z5 = tf.keras.layers.add([blk5,one_blk_3])\n",
        "\n",
        "# blk5_c = ResNetBlock(z5,128)\n",
        "# z5_c = tf.keras.layers.add([blk5_c,z5])\n",
        "\n",
        "\n",
        "\n",
        "avg_pool_layer = tf.keras.layers.GlobalAveragePooling2D()(z4_c)\n",
        "\n",
        "\n",
        "fc_layer = tf.keras.layers.Dense(10, activation='softmax')(avg_pool_layer)\n",
        "\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs= fc_layer)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0817 07:52:26.103640 140369041852288 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sV-nVdlMDwva",
        "outputId": "8cedf27f-43f8-47b3-a063-2fa68817f4f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 32)   0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 32)   128         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9216        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 16, 16, 32)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 32)   128         max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9216        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 32)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
            "                                                                 max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 32)   128         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 32)   9216        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 32)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 32)   9216        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 16, 16, 32)   0           conv2d_4[0][0]                   \n",
            "                                                                 add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout2d (SpatialDropo (None, 16, 16, 32)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 32)   128         spatial_dropout2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   18432       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   36864       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   2112        spatial_dropout2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 64)   0           conv2d_6[0][0]                   \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 64)   36864       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 64)   36864       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           conv2d_9[0][0]                   \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout2d_1 (SpatialDro (None, 16, 16, 64)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 64)   256         spatial_dropout2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 16, 16, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  73728       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 128)    512         max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 128)    0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  8320        spatial_dropout2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 128)    147456      activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 128)    0           conv2d_11[0][0]                  \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 128)    147456      activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 128)    512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 128)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 128)    147456      activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 128)    0           conv2d_14[0][0]                  \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 8, 8, 128)    0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 128)    512         dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 128)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 8, 8, 256)    294912      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 256)    0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 256)    1024        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 4, 4, 256)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 256)    33024       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 256)    589824      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 4, 4, 256)    0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 4, 4, 256)    0           conv2d_16[0][0]                  \n",
            "                                                                 max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 256)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 256)    589824      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 256)    1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 256)    589824      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 256)    0           conv2d_19[0][0]                  \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 256)          0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 2,800,170\n",
            "Trainable params: 2,796,714\n",
            "Non-trainable params: 3,456\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDMPrA1Gj01s"
      },
      "source": [
        "#Best LR would be 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-5l4Uuj2MLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for 24 epochs only\n",
        "\n",
        "MAX_LR= 0.15\n",
        "base_lr = 0.015\n",
        "\n",
        "def lr_func(epoch,lr):\n",
        "  lr = base_lr\n",
        "  #print(\"lwr\",tf.train.get_or_create_global_step())\n",
        "  max_lr = MAX_LR\n",
        "  \n",
        "  if(epoch == 0):\n",
        "    lr = base_lr\n",
        "  elif(epoch>0 and epoch<5):\n",
        "    lr += (max_lr-base_lr)*(epoch)/5\n",
        "  else:\n",
        "    lr = max_lr - (max_lr-base_lr)*(epoch-5)/18\n",
        "  print(\"final lr \",round(lr,5))\n",
        "  \n",
        "\n",
        "  return round(lr,5)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cRdnN8laTGJ",
        "colab_type": "code",
        "outputId": "94fec0ba-a779-4d28-e0d8-fe14f6a00de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aug_img.hdf5  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCMcOyj5cz-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = tf.keras.utils.HDF5Matrix('./aug_img.hdf5','dataset')\n",
        "label = tf.keras.utils.HDF5Matrix('./aug_img.hdf5','labels')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMpz9Oa8sQrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##from HDF5 ---\n",
        "import random\n",
        "\n",
        "train_data_aug_l = lambda i: (np.array(data[i:i+168]),np.array(label[i:i+168]))\n",
        "train_data_wo_l = lambda i: (np.array(train_features[i:i+168]),np.array(train_labels[i:i+168]))\n",
        "\n",
        "train_data_aug = lambda i: (np.array(data[i:i+256]),np.array(label[i:i+256]))\n",
        "train_data_wo = lambda i: (np.array(train_features[i:i+256]),np.array(train_labels[i:i+256]))\n",
        "\n",
        "num_seq = list(range(len(train_features)//512+1))\n",
        "\n",
        "\n",
        "def data_generator():\n",
        "  \n",
        "  train_data= np.zeros(shape=(50000,32,32,3))\n",
        "  train_lbl = np.zeros(shape=(50000,10))\n",
        "  \n",
        "  \n",
        "  tmp=0\n",
        "  for k in num_seq:\n",
        "    aug_ran = random.randrange(0,len(data)-256)\n",
        "    wo_aug = random.randrange(0,len(train_features)-256)\n",
        "\n",
        "    inital=tmp\n",
        "    if(k == 97):\n",
        "      final = inital+336\n",
        "      train_data[inital:final] = np.concatenate((train_data_aug_l(aug_ran)[0],train_data_wo_l(wo_aug)[0]),axis=0)\n",
        "      train_lbl[inital:final] = np.concatenate((train_data_aug_l(aug_ran)[1],train_data_wo_l(wo_aug)[1]),axis=0)\n",
        "    else:\n",
        "      final  = inital+512\n",
        "      train_data[inital:final] = np.concatenate((train_data_aug(aug_ran)[0],train_data_wo(wo_aug)[0]),axis=0)\n",
        "      train_lbl[inital:final] = np.concatenate((train_data_aug(aug_ran)[1],train_data_wo(wo_aug)[1]),axis=0)\n",
        "\n",
        "    tmp += 512\n",
        "  \n",
        "  return train_data,train_lbl\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNoxs2zSTpUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = data_generator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKx2xkNOTuZ0",
        "colab_type": "code",
        "outputId": "ea2e3b46-221b-4ae7-9fd3-7036d27fb9b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']\n",
        "plt.imshow(x[4])\n",
        "y[4]\n",
        "#x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0816 14:04:21.560283 140433647257472 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/S9nmdTZqPXARwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDesY5KBFGqY",
        "colab_type": "code",
        "outputId": "a4253448-a136-4fb3-df9d-faf39b194339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "\n",
        "def min_max_scaler(iterations, num_iterations, end_percentage, min_val, max_val, invert = False, triangle_tilt = 0.7):\n",
        "  non_slant_mid_cycle_id = int(num_iterations * ((1. - end_percentage)) / float(2))\n",
        "  mid_cycle_id = int(triangle_tilt*int(num_iterations * ((1. - end_percentage)) / float(2)))\n",
        "  value = 0\n",
        "  if iterations > 2 * non_slant_mid_cycle_id:\n",
        "    \n",
        "    \n",
        "      extra_iters = (iterations - 2 * non_slant_mid_cycle_id)\n",
        "      current_percentage = 1 - (1 - 1/10)*extra_iters/(num_iterations - 2 * non_slant_mid_cycle_id)\n",
        "            \n",
        "\n",
        "  elif iterations >  mid_cycle_id:\n",
        "      current_percentage = 1. - (iterations - mid_cycle_id) / (2*non_slant_mid_cycle_id - mid_cycle_id)\n",
        "      \n",
        "  else:\n",
        "      current_percentage = iterations / mid_cycle_id\n",
        "      \n",
        "  if invert:\n",
        "    if iterations > 2 * non_slant_mid_cycle_id:\n",
        "      return max_val\n",
        "    return max_val - current_percentage * (max_val - min_val)\n",
        "  else:\n",
        "    if iterations > 2 * non_slant_mid_cycle_id:\n",
        "      return min_val * current_percentage\n",
        "    return min_val + current_percentage * (max_val - min_val)\n",
        "  \n",
        "    \n",
        "\n",
        "print(\"=\"*80)\n",
        "scales = []\n",
        "for i in range(1000):\n",
        "  p = min_max_scaler(i,1000,0.1,min_val=0.1,max_val=1, invert=False,)\n",
        "  scales.append(p)\n",
        "  \n",
        "plt.plot(np.array(scales))\n",
        "plt.title(\"LR Graph\")\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n62RC7gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-j0zpmZFa0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class OneCycleLR(Callback):\n",
        "    def __init__(self,\n",
        "                 epochs,\n",
        "                 batch_size,\n",
        "                 samples,\n",
        "                 steps,\n",
        "                 max_lr,\n",
        "                 end_percentage=0.1,\n",
        "                 scale=100,\n",
        "                 maximum_momentum=0.95,\n",
        "                 minimum_momentum=0.85,\n",
        "                 triangle_tilt = 0.75,\n",
        "                 verbose=True):\n",
        "        \n",
        "        super(OneCycleLR, self).__init__()\n",
        "\n",
        "        if end_percentage < 0. or end_percentage > 1.:\n",
        "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
        "\n",
        "\n",
        "        self.initial_lr = max_lr\n",
        "        self.end_percentage = end_percentage\n",
        "        self.scale = scale\n",
        "        self.max_momentum = maximum_momentum\n",
        "        self.min_momentum = minimum_momentum\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if self.max_momentum is not None and self.min_momentum is not None:\n",
        "            self._update_momentum = True\n",
        "        else:\n",
        "            self._update_momentum = False\n",
        "\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.samples = samples\n",
        "        self.triangle_tilt=triangle_tilt\n",
        "        self.steps = None\n",
        "        self.num_iterations = None\n",
        "        self.mid_cycle_id = None\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Reset the callback.\n",
        "        \"\"\"\n",
        "        self.clr_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "    def compute_lr(self):\n",
        "        \n",
        "        new_lr = min_max_scaler(self.clr_iterations, self.num_iterations, self.end_percentage, self.initial_lr/self.scale, self.initial_lr, invert = False, triangle_tilt=self.triangle_tilt)\n",
        "        return new_lr\n",
        "\n",
        "    def compute_momentum(self):\n",
        "         \n",
        "        new_momentum = min_max_scaler(self.clr_iterations, self.num_iterations, self.end_percentage, self.min_momentum, self.max_momentum, invert = True, triangle_tilt=self.triangle_tilt)\n",
        "        return new_momentum\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.steps is not None:\n",
        "            self.num_iterations = self.epochs * self.steps\n",
        "        else:\n",
        "            if (self.samples % self.batch_size) == 0:\n",
        "                remainder = 0\n",
        "            else:\n",
        "                remainder = 1\n",
        "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
        "\n",
        "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
        "\n",
        "        self._reset()\n",
        "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        self.clr_iterations += 1\n",
        "        new_lr = self.compute_lr()\n",
        "\n",
        "        self.history.setdefault('lr', []).append(\n",
        "            K.get_value(self.model.optimizer.lr))\n",
        "        K.set_value(self.model.optimizer.lr, new_lr)\n",
        "\n",
        "        if self._update_momentum:\n",
        "            if not hasattr(self.model.optimizer, 'momentum'):\n",
        "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
        "\n",
        "            new_momentum = self.compute_momentum()\n",
        "\n",
        "            self.history.setdefault('momentum', []).append(\n",
        "                K.get_value(self.model.optimizer.momentum))\n",
        "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self.verbose:\n",
        "            if self._update_momentum:\n",
        "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
        "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
        "\n",
        "            else:\n",
        "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
        "                \n",
        "    \n",
        "    def plot(self):\n",
        "        plt.title(\"LR-Plot\")\n",
        "        plt.plot(self.history['lr'])\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"LR\")\n",
        "        plt.show()\n",
        "        \n",
        "        plt.title(\"Momentum-Plot\")\n",
        "        plt.plot(self.history['momentum'])\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Momentum\")\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J12BJOtqFFpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(nesterov=True)\n",
        "model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "upsNiPO4JWjb",
        "colab": {}
      },
      "source": [
        "# filepath = \"Resnet-13-test1.hdf5\"\n",
        "# from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
        "\n",
        "# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "# tmp_gen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "# ##Train the model\n",
        "# # model_info = model.fit_generator(train_generator,\n",
        "# #                                  steps_per_epoch=np.ceil(50000/128), epochs=24,  \n",
        "# #                                  validation_data = test_generator, verbose=1,callbacks=[checkpoint,LearningRateScheduler(lr_func, verbose=1)])\n",
        "\n",
        "# EPOCHS=24\n",
        "# for i in range(EPOCHS): \n",
        "#   X_train,X_label = data_generator()\n",
        "#   model.fit(X_train,X_label,batch_size=512,\n",
        "#                                epochs=1,validation_data=(test_features,test_labels),\n",
        "#                          callbacks=[LearningRateScheduler(lr_func, verbose=1)]\n",
        " #          )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cQmN6Lxeugu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import random\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, data, label, train_features,train_labels, batch_size=32, dim=(32,32), n_channels=3,\n",
        "                 n_classes=10, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.label = label\n",
        "        self.data = data\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.train_features = train_features\n",
        "        self.train_labels = train_labels\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        \n",
        "        # Generate indexes of the batch\n",
        "        #indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        aug_ran = random.randrange(0,len(self.data)-256)\n",
        "        wo_aug = random.randrange(0,len(self.train_features)-256)\n",
        "        #print(aug_ran)\n",
        "        train_data_aug = lambda i: (np.array(self.data[i:i+64]),np.array(self.label[i:i+64]))\n",
        "        train_data_wo = lambda i: (np.array(self.train_features[i:i+64]),np.array(self.train_labels[i:i+64]))\n",
        "        \n",
        "        \n",
        "        X = np.concatenate((train_data_aug(aug_ran)[0],train_data_wo(wo_aug)[0]),axis=0)\n",
        "        y = np.concatenate((train_data_aug(aug_ran)[1],train_data_wo(wo_aug)[1]),axis=0)\n",
        "        #print(\"label\",y.shape, \" \" ,X.shape)\n",
        "        #plt.imshow(X[0])\n",
        "        # Generate data\n",
        "        #X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.data))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD1_C-6ufxyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = DataGenerator(data,label,train_features,train_labels,128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AQ-_Y2jIuFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "olr = OneCycleLR(epochs=24,\n",
        "                 batch_size=128,\n",
        "                 samples=50000,steps=None,\n",
        "                 max_lr=0.2,triangle_tilt=0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6oYKAWatUYa",
        "colab_type": "code",
        "outputId": "8436d776-e676-4e26-d19d-358fa599b894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "x,y = test_generator.next()\n",
        "plt.imshow(x[76])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0817 07:52:54.207176 140369041852288 image.py:648] Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa9db1f4908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FU6T2uYD-Tn",
        "colab_type": "code",
        "outputId": "6a8cf39b-9826-48fa-beb5-cd3bc41f2ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
        "model.fit_generator(generator=gen,steps_per_epoch=np.ceil(50000/128), epochs=24,  \n",
        "                                validation_data = test_generator, verbose=1,callbacks =[olr])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 5.5343 - acc: 0.1663 - lr: 0.02711 - momentum: 0.94 \n",
            "391/391 [==============================] - 25s 64ms/step - loss: 5.5313 - acc: 0.1668 - val_loss: 4.9457 - val_acc: 0.2389\n",
            "Epoch 2/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 4.4681 - acc: 0.3022 - lr: 0.05229 - momentum: 0.92 \n",
            "391/391 [==============================] - 20s 51ms/step - loss: 4.4653 - acc: 0.3026 - val_loss: 3.8819 - val_acc: 0.3524\n",
            "Epoch 3/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 3.4108 - acc: 0.4017 - lr: 0.07747 - momentum: 0.91 \n",
            "391/391 [==============================] - 20s 51ms/step - loss: 3.4085 - acc: 0.4018 - val_loss: 3.0434 - val_acc: 0.4123\n",
            "Epoch 4/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 2.5277 - acc: 0.4812 - lr: 0.10264 - momentum: 0.90 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 2.5279 - acc: 0.4810 - val_loss: 2.9935 - val_acc: 0.3070\n",
            "Epoch 5/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.9954 - acc: 0.5257 - lr: 0.12782 - momentum: 0.89 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.9953 - acc: 0.5256 - val_loss: 2.3015 - val_acc: 0.4295\n",
            "Epoch 6/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.7521 - acc: 0.5476 - lr: 0.15300 - momentum: 0.87 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.7520 - acc: 0.5477 - val_loss: 1.7290 - val_acc: 0.5519\n",
            "Epoch 7/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.6929 - acc: 0.5532 - lr: 0.17817 - momentum: 0.86 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.6922 - acc: 0.5534 - val_loss: 1.9811 - val_acc: 0.5067\n",
            "Epoch 8/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.7121 - acc: 0.5514 - lr: 0.19820 - momentum: 0.85 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.7120 - acc: 0.5515 - val_loss: 1.7067 - val_acc: 0.5682\n",
            "Epoch 9/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.6744 - acc: 0.5711 - lr: 0.18465 - momentum: 0.86 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.6749 - acc: 0.5710 - val_loss: 1.8682 - val_acc: 0.5510\n",
            "Epoch 10/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.6349 - acc: 0.5825 - lr: 0.17110 - momentum: 0.86 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.6346 - acc: 0.5827 - val_loss: 1.7071 - val_acc: 0.5721\n",
            "Epoch 11/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.5822 - acc: 0.6034 - lr: 0.15754 - momentum: 0.87 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.5820 - acc: 0.6034 - val_loss: 1.6049 - val_acc: 0.5990\n",
            "Epoch 12/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.5212 - acc: 0.6186 - lr: 0.14399 - momentum: 0.88 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.5209 - acc: 0.6187 - val_loss: 1.4626 - val_acc: 0.6374\n",
            "Epoch 13/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.4719 - acc: 0.6346 - lr: 0.13044 - momentum: 0.89 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.4714 - acc: 0.6347 - val_loss: 1.6019 - val_acc: 0.6149\n",
            "Epoch 14/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.4384 - acc: 0.6439 - lr: 0.11689 - momentum: 0.89 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.4384 - acc: 0.6438 - val_loss: 1.2761 - val_acc: 0.7053\n",
            "Epoch 15/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.3927 - acc: 0.6581 - lr: 0.10334 - momentum: 0.90 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.3925 - acc: 0.6581 - val_loss: 1.4891 - val_acc: 0.6347\n",
            "Epoch 16/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.3360 - acc: 0.6764 - lr: 0.08979 - momentum: 0.91 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.3354 - acc: 0.6766 - val_loss: 1.4584 - val_acc: 0.6298\n",
            "Epoch 17/24\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.2824 - acc: 0.6916 - lr: 0.07624 - momentum: 0.91 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.2823 - acc: 0.6916 - val_loss: 1.3951 - val_acc: 0.6700\n",
            "Epoch 18/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.2222 - acc: 0.7092 - lr: 0.06269 - momentum: 0.92 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.2229 - acc: 0.7090 - val_loss: 1.5016 - val_acc: 0.6498\n",
            "Epoch 19/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.1869 - acc: 0.7248 - lr: 0.04913 - momentum: 0.93 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.1869 - acc: 0.7248 - val_loss: 1.2683 - val_acc: 0.7049\n",
            "Epoch 20/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 1.0797 - acc: 0.7517 - lr: 0.03558 - momentum: 0.93 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 1.0789 - acc: 0.7519 - val_loss: 1.1498 - val_acc: 0.7362\n",
            "Epoch 21/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.9771 - acc: 0.7777 - lr: 0.02203 - momentum: 0.94 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.9768 - acc: 0.7778 - val_loss: 1.0427 - val_acc: 0.7644\n",
            "Epoch 22/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.8861 - acc: 0.8038 - lr: 0.00848 - momentum: 0.95 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.8856 - acc: 0.8040 - val_loss: 0.9214 - val_acc: 0.7966\n",
            "Epoch 23/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.8077 - acc: 0.8283 - lr: 0.00162 - momentum: 0.95 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.8075 - acc: 0.8283 - val_loss: 0.8859 - val_acc: 0.8069\n",
            "Epoch 24/24\n",
            "389/391 [============================>.] - ETA: 0s - loss: 0.7682 - acc: 0.8395 - lr: 0.00090 - momentum: 0.95 \n",
            "391/391 [==============================] - 20s 52ms/step - loss: 0.7682 - acc: 0.8395 - val_loss: 0.8743 - val_acc: 0.8103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa9db1b8be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l6KO5KGbfHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}