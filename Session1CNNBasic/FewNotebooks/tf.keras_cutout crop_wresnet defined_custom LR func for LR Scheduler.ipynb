{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deepak14.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "saYEjNp5C0Td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from keras.utils import np_utils\n",
        "import cv2\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_7_9AhqDJH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "num_train, img_channels, img_rows, img_cols =  train_features.shape\n",
        "num_test, _, _, _ =  test_features.shape\n",
        "num_classes = len(np.unique(train_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buvizArXDL0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = train_features.astype('float32')/255\n",
        "test_features = test_features.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
        "test_labels = np_utils.to_categorical(test_labels, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQouC-aZDPAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cutout_eraser_and_random_crop(p=0.5,s_l=0.05,s_h=0.3,r_1=0.3,r_2=1/0.3,max_erasers_per_image=1,pixel_level=True,random_crop_size=(32,32),padding_pixels=4):\n",
        "  \n",
        "  assert max_erasers_per_image>=1 \n",
        "  def eraser(input_img):\n",
        "        v_l = np.min(input_img)\n",
        "        v_h = np.max(input_img)\n",
        "        img_h, img_w, img_c = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "        mx = np.random.randint(1,max_erasers_per_image+1)\n",
        "        for i in range(mx):\n",
        "          while True:\n",
        "              s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "              r = np.random.uniform(r_1, r_2)\n",
        "              w = int(np.sqrt(s / r))\n",
        "              h = int(np.sqrt(s * r))\n",
        "              left = np.random.randint(0, img_w)\n",
        "              top = np.random.randint(0, img_h)\n",
        "\n",
        "              if left + w <= img_w and top + h <= img_h:\n",
        "                  break\n",
        "\n",
        "          if pixel_level:\n",
        "              c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "          else:\n",
        "              c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "          input_img[top:top + h, left:left + w, :] = c\n",
        "        return input_img\n",
        "\n",
        "    \n",
        "  def random_crop(input_image):\n",
        "    assert input_image.shape[2]==3\n",
        "\n",
        "    #pad for 4 pixels\n",
        "    img = cv2.copyMakeBorder(input_image,padding_pixels,padding_pixels,padding_pixels,padding_pixels,cv2.BORDER_REPLICATE)\n",
        "    height , width =img.shape[0],img.shape[1]\n",
        "    dy,dx = random_crop_size\n",
        "    x = np.random.randint(0,width - dx + 1)\n",
        "    y = np.random.randint(0,height - dy + 1)\n",
        "    return img[y:(y+dy),x:(x+dx),:]\n",
        "\n",
        "  def preprocess_image(input_image):\n",
        "    return eraser(random_crop(input_image))\n",
        "  \n",
        "  return preprocess_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seLnbzBMDSGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=0.5,featurewise_center=True, featurewise_std_normalization=True,preprocessing_function=get_cutout_eraser_and_random_crop())\n",
        "datagen.mean = np.array([0.4914, 0.4822, 0.4465], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "datagen.std = np.array([0.2023, 0.1994, 0.2010], dtype=np.float32).reshape((1,1,3)) # ordering: [R, G, B]\n",
        "#datagen.fit(train_features)\n",
        "train_generator = datagen.flow(train_features,train_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmERfb8gDWLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "test_datagen.fit(test_features)\n",
        "test_generator = test_datagen.flow(test_features,test_labels,batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi8rziwYDY9q",
        "colab_type": "code",
        "outputId": "fce1cb2e-f44d-425e-fab1-129ebd7f2ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# import time, math\n",
        "# def init_pytorch(shape, dtype=tf.float32, partition_info=None):\n",
        "#   fan = np.prod(shape[:-1])\n",
        "#   bound = 1 / math.sqrt(fan)\n",
        "#   return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)\n",
        "\n",
        "initializer = tf.keras.initializers.glorot_normal(seed=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0807 14:11:22.780110 140594319284096 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTa07xf6QwyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def WResNetBlock(input_layer,channels,stride=1):\n",
        "  bn_1 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(input_layer)\n",
        "  activation_layer_b1 = tf.keras.layers.Activation('relu')(bn_1)\n",
        "  block_layer_1 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',kernel_initializer=initializer,use_bias=False,strides=stride)(activation_layer_b1)\n",
        "  bn_2 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(block_layer_1)\n",
        "  activation_layer_b2 = tf.keras.layers.Activation('relu')(bn_2)\n",
        "  block_layer_2 = tf.keras.layers.Conv2D(channels, (3,3), padding='same',kernel_initializer=initializer,use_bias=False,strides=1)(activation_layer_b2)\n",
        "  \n",
        "  return block_layer_2, activation_layer_b1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5DS302dSAMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tf.keras.layers import Input, add, GlobalAveragePooling2D, Dense\n",
        "#from tf.keras.models import Model\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
        "x1 = tf.keras.layers.Conv2D(16 ,(3, 3),padding='same',kernel_initializer=initializer,use_bias=False)(inputs)   #32x32 \n",
        "\n",
        "#FIRST BLOCK\n",
        "blk1,ack1 = WResNetBlock(x1,96) \n",
        "ack1add = tf.keras.layers.Conv2D(96, (1, 1), padding='same',kernel_initializer=initializer,use_bias=False)(ack1)\n",
        "fb1 = tf.keras.layers.add([ack1add,blk1])\n",
        "\n",
        "#SECOND BLOCK\n",
        "blk2,ack2 = WResNetBlock(fb1,96) \n",
        "fb2 = tf.keras.layers.add([blk2,ack2])\n",
        "\n",
        "#THIRD BLOCK\n",
        "blk3,ack3 = WResNetBlock(fb2,96) \n",
        "fb3 = tf.keras.layers.add([blk3,ack3])\n",
        "\n",
        "\n",
        "#FOURTH BLOCK\n",
        "blk4,ack4 = WResNetBlock(fb3,192,2) \n",
        "ack4add = tf.keras.layers.Conv2D(192, (1, 1), padding='same',kernel_initializer=initializer,strides=(2,2),use_bias=False)(ack4)\n",
        "fb4 = tf.keras.layers.add([blk4,ack4add])\n",
        "\n",
        "\n",
        "#FIFTH BLOCK\n",
        "blk5,ack5 = WResNetBlock(fb4,192) \n",
        "fb5 = tf.keras.layers.add([blk5,ack5])\n",
        "\n",
        "#SIXTH BLOCK\n",
        "blk6,ack6 = WResNetBlock(fb5,192) \n",
        "fb6 = tf.keras.layers.add([blk6,ack6])\n",
        "\n",
        "#SEVENTH BLOCK\n",
        "blk7,ack7 = WResNetBlock(fb6,384,2) \n",
        "ack7add = tf.keras.layers.Conv2D(384, (1, 1), padding='same',kernel_initializer=initializer,strides=(2,2),use_bias=False)(ack7)\n",
        "fb7 = tf.keras.layers.add([blk7,ack7add])\n",
        "\n",
        "#EIGHTH BLOCK\n",
        "blk8,ack8 = WResNetBlock(fb7,384) \n",
        "fb8 = tf.keras.layers.add([blk8,ack8])\n",
        "\n",
        "#NINTH BLOCK\n",
        "blk9,ack9 = WResNetBlock(fb8,384) \n",
        "fb9 = tf.keras.layers.add([blk9,ack9])\n",
        "\n",
        "bn_10 = tf.keras.layers.BatchNormalization(momentum=0.9,epsilon=1e-5)(fb9)\n",
        "ack10 = tf.keras.layers.Activation('relu')(bn_10)\n",
        "avgpool = tf.keras.layers.GlobalAveragePooling2D()(ack10)\n",
        "\n",
        "flatten_layer = tf.keras.layers.Flatten()(avgpool)\n",
        "\n",
        "fc_layer = tf.keras.layers.Dense(10, activation='softmax')(flatten_layer)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs= fc_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX2uGSpgDwMA",
        "colab_type": "code",
        "outputId": "986a87d5-b456-4d9b-c76d-09626e00129f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 32, 32, 16)   432         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 32, 32, 16)   64          conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 32, 32, 16)   0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 32, 32, 96)   13824       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 32, 32, 96)   384         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 32, 32, 96)   0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 32, 32, 96)   1536        activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 32, 32, 96)   82944       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_36 (Add)                    (None, 32, 32, 96)   0           conv2d_91[0][0]                  \n",
            "                                                                 conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 32, 32, 96)   384         add_36[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 32, 32, 96)   0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 32, 32, 96)   82944       activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 32, 32, 96)   384         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 32, 32, 96)   0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 32, 32, 96)   82944       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 32, 32, 96)   0           conv2d_93[0][0]                  \n",
            "                                                                 activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 32, 32, 96)   384         add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 32, 32, 96)   0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 32, 32, 96)   82944       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 32, 32, 96)   384         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 32, 32, 96)   0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 32, 32, 96)   82944       activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 32, 32, 96)   0           conv2d_95[0][0]                  \n",
            "                                                                 activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 32, 32, 96)   384         add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 32, 32, 96)   0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 16, 16, 192)  165888      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 16, 16, 192)  768         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 16, 16, 192)  0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 16, 16, 192)  331776      activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 16, 16, 192)  18432       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 16, 16, 192)  0           conv2d_97[0][0]                  \n",
            "                                                                 conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 16, 16, 192)  768         add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 16, 16, 192)  0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 16, 16, 192)  331776      activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 16, 16, 192)  768         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 16, 16, 192)  0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 16, 16, 192)  331776      activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 16, 16, 192)  0           conv2d_100[0][0]                 \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 16, 16, 192)  768         add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 16, 16, 192)  0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 16, 16, 192)  331776      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 16, 16, 192)  768         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 16, 16, 192)  0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 16, 16, 192)  331776      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 16, 16, 192)  0           conv2d_102[0][0]                 \n",
            "                                                                 activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 16, 16, 192)  768         add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 16, 16, 192)  0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 8, 8, 384)    663552      activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 8, 8, 384)    1536        conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 8, 8, 384)    1327104     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 8, 8, 384)    73728       activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 8, 8, 384)    0           conv2d_104[0][0]                 \n",
            "                                                                 conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 8, 8, 384)    1536        add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 8, 8, 384)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 8, 8, 384)    1327104     activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1536        conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 8, 8, 384)    1327104     activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 8, 8, 384)    0           conv2d_107[0][0]                 \n",
            "                                                                 activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1536        add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 8, 8, 384)    1327104     activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 8, 8, 384)    1536        conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 8, 8, 384)    1327104     activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_44 (Add)                    (None, 8, 8, 384)    0           conv2d_109[0][0]                 \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 8, 8, 384)    1536        add_44[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 8, 8, 384)    0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 384)          0           activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 384)          0           global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           3850        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 9,666,554\n",
            "Trainable params: 9,658,458\n",
            "Non-trainable params: 8,096\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Epx3HuDzbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #from one_cycle_lr import LRFinder\n",
        "# from one_cycle_lr_tf import LRFinder\n",
        "# num_samples= train_features.shape[0]\n",
        "# batch_size =512\n",
        "# num_epoch=50\n",
        "# max_lr=0.05\n",
        "\n",
        "#Best LR would be 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl6a1_veD2BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def truncate(n, decimals=0):\n",
        "#     multiplier = 10 ** decimals\n",
        "#     return int(n * multiplier) / multiplier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Olnwf5OsD6ww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## for 24 epochs only\n",
        "\n",
        "MAX_LR= 0.05\n",
        "base_lr = 0.01\n",
        "\n",
        "def lr_func(epoch,lr):\n",
        "  lr = base_lr\n",
        "  max_lr = MAX_LR\n",
        "  \n",
        "  if(epoch == 0):\n",
        "    lr = base_lr\n",
        "  elif(epoch>0 and epoch<11):\n",
        "    lr += (max_lr-base_lr)*(epoch)/11\n",
        "  else:\n",
        "    lr = max_lr - (max_lr-base_lr)*(epoch-11)/18\n",
        "  print(\"final lr \",round(lr,5))\n",
        "  return round(lr,5)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osc_AWz7D_4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7gpxQ1SEBHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# num_samples= train_features.shape[0]\n",
        "# batch_size =128\n",
        "# num_epoch=24\n",
        "# max_lr=0.1\n",
        "\n",
        "# from one_cycle_lr_tf import OneCycleLR\n",
        "\n",
        "# lr_manager = OneCycleLR(num_samples, num_epoch, batch_size, max_lr,\n",
        "#                         end_percentage=0.1, scale_percentage=None,\n",
        "#                         maximum_momentum=0.95, minimum_momentum=0.85)\n",
        "\n",
        "# opt = tf.keras.optimizers.SGD()\n",
        "# model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=['accuracy'])                   maximum_momentum=0.95, minimum_momentum=0.85)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5KBmk3UAJLD",
        "colab_type": "code",
        "outputId": "8304e971-8c8a-4714-a304-64ebbee06145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "filepath = \"Resnet-13-test1.hdf5\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "##Train the model\n",
        "model_info = model.fit_generator(train_generator,\n",
        "                                 steps_per_epoch=np.ceil(50000/128), epochs=30,  \n",
        "                                 validation_data = test_generator, verbose=1,callbacks=[checkpoint,LearningRateScheduler(lr_func, verbose=1)])\n",
        "\n",
        "\n",
        "# model_info = model.fit_generator(train_generator,\n",
        "#                                  steps_per_epoch=np.ceil(50000/128), epochs=24,  \n",
        "#                                  validation_data = test_generator, verbose=1,callbacks=[checkpoint,lr_manager])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final lr  0.01\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 1/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.4681 - acc: 0.4660\n",
            "Epoch 00001: val_acc improved from -inf to 0.54090, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 206s 527ms/step - loss: 1.4674 - acc: 0.4663 - val_loss: 1.2488 - val_acc: 0.5409\n",
            "final lr  0.01364\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.01364.\n",
            "Epoch 2/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 1.0074 - acc: 0.6417\n",
            "Epoch 00002: val_acc improved from 0.54090 to 0.66360, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 1.0073 - acc: 0.6417 - val_loss: 0.9715 - val_acc: 0.6636\n",
            "final lr  0.01727\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.01727.\n",
            "Epoch 3/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.8117 - acc: 0.7139\n",
            "Epoch 00003: val_acc improved from 0.66360 to 0.76730, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.8114 - acc: 0.7140 - val_loss: 0.6796 - val_acc: 0.7673\n",
            "final lr  0.02091\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.02091.\n",
            "Epoch 4/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.7583\n",
            "Epoch 00004: val_acc improved from 0.76730 to 0.78050, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 497ms/step - loss: 0.6925 - acc: 0.7584 - val_loss: 0.6557 - val_acc: 0.7805\n",
            "final lr  0.02455\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.02455.\n",
            "Epoch 5/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.6199 - acc: 0.7858\n",
            "Epoch 00005: val_acc improved from 0.78050 to 0.78360, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.6197 - acc: 0.7859 - val_loss: 0.6324 - val_acc: 0.7836\n",
            "final lr  0.02818\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.02818.\n",
            "Epoch 6/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5582 - acc: 0.8060\n",
            "Epoch 00006: val_acc did not improve from 0.78360\n",
            "391/391 [==============================] - 194s 495ms/step - loss: 0.5579 - acc: 0.8061 - val_loss: 0.6950 - val_acc: 0.7682\n",
            "final lr  0.03182\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.03182.\n",
            "Epoch 7/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.5100 - acc: 0.8225\n",
            "Epoch 00007: val_acc improved from 0.78360 to 0.82220, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.5097 - acc: 0.8226 - val_loss: 0.5298 - val_acc: 0.8222\n",
            "final lr  0.03545\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.03545.\n",
            "Epoch 8/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4698 - acc: 0.8365\n",
            "Epoch 00008: val_acc did not improve from 0.82220\n",
            "391/391 [==============================] - 194s 495ms/step - loss: 0.4696 - acc: 0.8365 - val_loss: 0.5443 - val_acc: 0.8196\n",
            "final lr  0.03909\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.03909.\n",
            "Epoch 9/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4409 - acc: 0.8457\n",
            "Epoch 00009: val_acc improved from 0.82220 to 0.85090, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.4409 - acc: 0.8457 - val_loss: 0.4655 - val_acc: 0.8509\n",
            "final lr  0.04273\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.04273.\n",
            "Epoch 10/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.4198 - acc: 0.8550\n",
            "Epoch 00010: val_acc improved from 0.85090 to 0.85260, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 497ms/step - loss: 0.4196 - acc: 0.8551 - val_loss: 0.4496 - val_acc: 0.8526\n",
            "final lr  0.04636\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.04636.\n",
            "Epoch 11/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8652\n",
            "Epoch 00011: val_acc improved from 0.85260 to 0.86490, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.3874 - acc: 0.8653 - val_loss: 0.4233 - val_acc: 0.8649\n",
            "final lr  0.05\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.05.\n",
            "Epoch 12/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3662 - acc: 0.8717\n",
            "Epoch 00012: val_acc did not improve from 0.86490\n",
            "391/391 [==============================] - 194s 495ms/step - loss: 0.3662 - acc: 0.8717 - val_loss: 0.4455 - val_acc: 0.8578\n",
            "final lr  0.04778\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.04778.\n",
            "Epoch 13/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8865\n",
            "Epoch 00013: val_acc improved from 0.86490 to 0.87590, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.3273 - acc: 0.8866 - val_loss: 0.3764 - val_acc: 0.8759\n",
            "final lr  0.04556\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.04556.\n",
            "Epoch 14/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2973 - acc: 0.8966\n",
            "Epoch 00014: val_acc improved from 0.87590 to 0.89580, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.2975 - acc: 0.8965 - val_loss: 0.3126 - val_acc: 0.8958\n",
            "final lr  0.04333\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.04333.\n",
            "Epoch 15/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9023\n",
            "Epoch 00015: val_acc did not improve from 0.89580\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.2744 - acc: 0.9023 - val_loss: 0.3360 - val_acc: 0.8883\n",
            "final lr  0.04111\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.04111.\n",
            "Epoch 16/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9137\n",
            "Epoch 00016: val_acc did not improve from 0.89580\n",
            "391/391 [==============================] - 193s 495ms/step - loss: 0.2433 - acc: 0.9138 - val_loss: 0.3331 - val_acc: 0.8932\n",
            "final lr  0.03889\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.03889.\n",
            "Epoch 17/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2281 - acc: 0.9192\n",
            "Epoch 00017: val_acc did not improve from 0.89580\n",
            "391/391 [==============================] - 193s 495ms/step - loss: 0.2282 - acc: 0.9192 - val_loss: 0.3276 - val_acc: 0.8936\n",
            "final lr  0.03667\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.03667.\n",
            "Epoch 18/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.2067 - acc: 0.9273\n",
            "Epoch 00018: val_acc improved from 0.89580 to 0.90830, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.2067 - acc: 0.9272 - val_loss: 0.3009 - val_acc: 0.9083\n",
            "final lr  0.03444\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.03444.\n",
            "Epoch 19/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.9315\n",
            "Epoch 00019: val_acc did not improve from 0.90830\n",
            "391/391 [==============================] - 194s 495ms/step - loss: 0.1956 - acc: 0.9314 - val_loss: 0.2944 - val_acc: 0.9081\n",
            "final lr  0.03222\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.03222.\n",
            "Epoch 20/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1743 - acc: 0.9376\n",
            "Epoch 00020: val_acc improved from 0.90830 to 0.91210, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 495ms/step - loss: 0.1741 - acc: 0.9377 - val_loss: 0.2828 - val_acc: 0.9121\n",
            "final lr  0.03\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.03.\n",
            "Epoch 21/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9435\n",
            "Epoch 00021: val_acc improved from 0.91210 to 0.91920, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 495ms/step - loss: 0.1640 - acc: 0.9435 - val_loss: 0.2516 - val_acc: 0.9192\n",
            "final lr  0.02778\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.02778.\n",
            "Epoch 22/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9457\n",
            "Epoch 00022: val_acc did not improve from 0.91920\n",
            "391/391 [==============================] - 194s 495ms/step - loss: 0.1517 - acc: 0.9457 - val_loss: 0.2684 - val_acc: 0.9162\n",
            "final lr  0.02556\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.02556.\n",
            "Epoch 23/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9510\n",
            "Epoch 00023: val_acc improved from 0.91920 to 0.92250, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.1398 - acc: 0.9510 - val_loss: 0.2504 - val_acc: 0.9225\n",
            "final lr  0.02333\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.02333.\n",
            "Epoch 24/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1242 - acc: 0.9566\n",
            "Epoch 00024: val_acc did not improve from 0.92250\n",
            "391/391 [==============================] - 193s 495ms/step - loss: 0.1241 - acc: 0.9566 - val_loss: 0.2918 - val_acc: 0.9151\n",
            "final lr  0.02111\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.02111.\n",
            "Epoch 25/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1168 - acc: 0.9589\n",
            "Epoch 00025: val_acc improved from 0.92250 to 0.92710, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 495ms/step - loss: 0.1169 - acc: 0.9589 - val_loss: 0.2563 - val_acc: 0.9271\n",
            "final lr  0.01889\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.01889.\n",
            "Epoch 26/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9627\n",
            "Epoch 00026: val_acc improved from 0.92710 to 0.93160, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.1068 - acc: 0.9627 - val_loss: 0.2315 - val_acc: 0.9316\n",
            "final lr  0.01667\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.01667.\n",
            "Epoch 27/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9658\n",
            "Epoch 00027: val_acc did not improve from 0.93160\n",
            "391/391 [==============================] - 193s 495ms/step - loss: 0.0961 - acc: 0.9659 - val_loss: 0.2299 - val_acc: 0.9306\n",
            "final lr  0.01444\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.01444.\n",
            "Epoch 28/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.0895 - acc: 0.9692\n",
            "Epoch 00028: val_acc improved from 0.93160 to 0.93460, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.0895 - acc: 0.9692 - val_loss: 0.2302 - val_acc: 0.9346\n",
            "final lr  0.01222\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.01222.\n",
            "Epoch 29/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.0803 - acc: 0.9724\n",
            "Epoch 00029: val_acc did not improve from 0.93460\n",
            "391/391 [==============================] - 193s 495ms/step - loss: 0.0803 - acc: 0.9725 - val_loss: 0.2305 - val_acc: 0.9327\n",
            "final lr  0.01\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.01.\n",
            "Epoch 30/30\n",
            "390/391 [============================>.] - ETA: 0s - loss: 0.0759 - acc: 0.9740\n",
            "Epoch 00030: val_acc improved from 0.93460 to 0.93550, saving model to Resnet-13-test1.hdf5\n",
            "391/391 [==============================] - 194s 496ms/step - loss: 0.0758 - acc: 0.9740 - val_loss: 0.2280 - val_acc: 0.9355\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
